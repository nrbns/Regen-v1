{"version":3,"file":"pageExtractor-ZG_aqhIg.js","sources":["../../src/utils/pageExtractor.ts"],"sourcesContent":["/**\n * Page Extractor - Extract clean text and metadata from DOM\n * Handles structured content, tables, and removes noise\n * \n * Simple utility function for \"Ask about this page\" feature\n */\n\n/**\n * Simple page context extractor for \"Ask about this page\"\n * Returns basic title, URL, and text content\n * \n * Security: Sanitizes output to prevent XSS and limits content size\n */\nexport function extractPageContext(): { title: string; url: string; text: string } {\n  // Security: Limit text extraction to prevent memory issues\n  const MAX_TEXT_LENGTH = 50000; // 50KB limit\n  const MAX_TITLE_LENGTH = 200;\n  \n  let text = '';\n  let charCount = 0;\n  \n  try {\n    const walker = document.createTreeWalker(\n      document.body,\n      NodeFilter.SHOW_TEXT,\n      {\n        acceptNode: () => {\n          // Stop if we've reached the limit\n          if (charCount >= MAX_TEXT_LENGTH) {\n            return NodeFilter.FILTER_REJECT;\n          }\n          return NodeFilter.FILTER_ACCEPT;\n        },\n      }\n    );\n    \n    let node: Node | null;\n    while ((node = walker.nextNode()) && charCount < MAX_TEXT_LENGTH) {\n      const t = (node.textContent || '').trim();\n      if (t) {\n        const remaining = MAX_TEXT_LENGTH - charCount;\n        const toAdd = t.slice(0, remaining);\n        text += toAdd + '\\n';\n        charCount += toAdd.length + 1; // +1 for newline\n        \n        if (charCount >= MAX_TEXT_LENGTH) break;\n      }\n    }\n  } catch (error) {\n    // Fail gracefully if DOM access fails\n    console.warn('[PageExtractor] Failed to extract page context:', error);\n    text = '';\n  }\n\n  // Sanitize title and URL\n  const title = (document.title || '').slice(0, MAX_TITLE_LENGTH);\n  const url = window.location.href.slice(0, 2048); // URL length limit\n\n  return {\n    title,\n    url,\n    text: text.slice(0, MAX_TEXT_LENGTH), // Final safety check\n  };\n}\n\nexport interface PageMetadata {\n  title: string;\n  description?: string;\n  url: string;\n  headings: Array<{ level: number; text: string }>;\n  mainContent: string;\n  tables?: Array<{ headers: string[]; rows: string[][] }>;\n  links?: Array<{ text: string; url: string }>;\n  images?: Array<{ alt?: string; src: string }>;\n  author?: string;\n  publishedDate?: string;\n  wordCount: number;\n  estimatedReadTime: number; // in minutes\n}\n\n/**\n * Extract clean text from an element, removing script/style tags\n */\nfunction extractCleanText(element: Element | Document): string {\n  // Clone to avoid modifying the original\n  const clone = element.cloneNode(true) as Element;\n  \n  // Remove script and style elements\n  const scripts = clone.querySelectorAll('script, style, noscript, iframe, embed, object');\n  scripts.forEach(el => el.remove());\n  \n  // Get text content and clean it up\n  let text = clone.textContent || '';\n  \n  // Normalize whitespace\n  text = text.replace(/\\s+/g, ' ').trim();\n  \n  // Remove common noise patterns\n  text = text.replace(/\\[.*?\\]/g, ''); // Remove [bracketed content]\n  text = text.replace(/\\(.*?\\)/g, ''); // Remove (parentheses)\n  text = text.replace(/\\s+/g, ' '); // Normalize again\n  \n  return text.trim();\n}\n\n/**\n * Extract headings with their hierarchy\n */\nfunction extractHeadings(document: Document): Array<{ level: number; text: string }> {\n  const headings: Array<{ level: number; text: string }> = [];\n  const headingElements = document.querySelectorAll('h1, h2, h3, h4, h5, h6');\n  \n  for (const heading of Array.from(headingElements)) {\n    const level = parseInt(heading.tagName.charAt(1));\n    const text = extractCleanText(heading);\n    if (text) {\n      headings.push({ level, text });\n    }\n  }\n  \n  return headings;\n}\n\n/**\n * Extract structured tables\n */\nfunction extractTables(document: Document): Array<{ headers: string[]; rows: string[][] }> {\n  const tables: Array<{ headers: string[]; rows: string[][] }> = [];\n  const tableElements = document.querySelectorAll('table');\n  \n  for (const table of Array.from(tableElements)) {\n    const headers: string[] = [];\n    const rows: string[][] = [];\n    \n    // Extract headers from thead or first tr\n    const thead = table.querySelector('thead');\n    if (thead) {\n      const headerRow = thead.querySelector('tr');\n      if (headerRow) {\n        const cells = headerRow.querySelectorAll('th, td');\n        for (const cell of Array.from(cells)) {\n          headers.push(extractCleanText(cell));\n        }\n      }\n    } else {\n      // Try first row as headers\n      const firstRow = table.querySelector('tr');\n      if (firstRow) {\n        const cells = firstRow.querySelectorAll('th, td');\n        for (const cell of Array.from(cells)) {\n          headers.push(extractCleanText(cell));\n        }\n      }\n    }\n    \n    // Extract data rows\n    const tbody = table.querySelector('tbody') || table;\n    const dataRows = tbody.querySelectorAll('tr');\n    \n    for (const row of Array.from(dataRows)) {\n      // Skip header row if we already extracted headers\n      if (thead && row.parentElement === thead) continue;\n      \n      const cells = row.querySelectorAll('td');\n      if (cells.length > 0) {\n        const rowData = Array.from(cells).map((cell: Element) => extractCleanText(cell));\n        rows.push(rowData);\n      }\n    }\n    \n    if (headers.length > 0 || rows.length > 0) {\n      tables.push({ headers, rows });\n    }\n  }\n  \n  return tables;\n}\n\n/**\n * Extract links with their text\n */\nfunction extractLinks(document: Document): Array<{ text: string; url: string }> {\n  const links: Array<{ text: string; url: string }> = [];\n  const linkElements = document.querySelectorAll('a[href]');\n  \n  for (const link of Array.from(linkElements)) {\n    const href = link.getAttribute('href');\n    const text = extractCleanText(link);\n    \n    if (href && text) {\n      // Resolve relative URLs\n      try {\n        const url = new URL(href, document.location.href).href;\n        links.push({ text, url });\n      } catch {\n        // Invalid URL, skip\n      }\n    }\n  }\n  \n  return links;\n}\n\n/**\n * Extract images with alt text\n */\nfunction extractImages(document: Document): Array<{ alt?: string; src: string }> {\n  const images: Array<{ alt?: string; src: string }> = [];\n  const imageElements = document.querySelectorAll('img[src]');\n  \n  for (const img of Array.from(imageElements)) {\n    const src = img.getAttribute('src');\n    const alt = img.getAttribute('alt') || undefined;\n    \n    if (src) {\n      // Resolve relative URLs\n      try {\n        const url = new URL(src, document.location.href).href;\n        images.push({ alt, src: url });\n      } catch {\n        // Invalid URL, skip\n      }\n    }\n  }\n  \n  return images;\n}\n\n/**\n * Find the main content area using common patterns\n */\nfunction findMainContent(document: Document): Element | null {\n  // Try common semantic elements first\n  const semanticSelectors = [\n    'main',\n    'article',\n    '[role=\"main\"]',\n    '.content',\n    '.main-content',\n    '.post-content',\n    '.entry-content',\n    '#content',\n    '#main-content',\n  ];\n  \n  for (const selector of semanticSelectors) {\n    const element = document.querySelector(selector);\n    if (element) {\n      return element;\n    }\n  }\n  \n  // Fallback: find the largest non-nav/footer element\n  const body = document.body;\n  if (!body) return null;\n  \n  let largestElement: Element | null = null;\n  let largestSize = 0;\n  \n  const candidates = body.querySelectorAll('div, section');\n  for (const candidate of Array.from(candidates)) {\n    // Skip navigation, footer, header\n    const role = candidate.getAttribute('role');\n    const className = candidate.className.toLowerCase();\n    const id = candidate.id.toLowerCase();\n    \n    if (\n      role === 'navigation' ||\n      role === 'footer' ||\n      role === 'header' ||\n      className.includes('nav') ||\n      className.includes('footer') ||\n      className.includes('header') ||\n      id.includes('nav') ||\n      id.includes('footer') ||\n      id.includes('header')\n    ) {\n      continue;\n    }\n    \n    const text = extractCleanText(candidate);\n    if (text.length > largestSize) {\n      largestSize = text.length;\n      largestElement = candidate;\n    }\n  }\n  \n  return largestElement || body;\n}\n\n/**\n * Extract metadata from meta tags\n */\nfunction extractMetaTags(document: Document): {\n  description?: string;\n  author?: string;\n  publishedDate?: string;\n} {\n  const meta: {\n    description?: string;\n    author?: string;\n    publishedDate?: string;\n  } = {};\n  \n  // Description\n  const descTag = document.querySelector('meta[name=\"description\"], meta[property=\"og:description\"]');\n  if (descTag) {\n    meta.description = descTag.getAttribute('content') || undefined;\n  }\n  \n  // Author\n  const authorTag = document.querySelector('meta[name=\"author\"], meta[name=\"article:author\"]');\n  if (authorTag) {\n    meta.author = authorTag.getAttribute('content') || undefined;\n  }\n  \n  // Published date\n  const dateTag = document.querySelector('meta[name=\"article:published_time\"], meta[property=\"article:published_time\"], time[datetime]');\n  if (dateTag) {\n    meta.publishedDate = dateTag.getAttribute('datetime') || dateTag.getAttribute('content') || undefined;\n  }\n  \n  return meta;\n}\n\n/**\n * Calculate estimated read time (average 200 words per minute)\n */\nfunction calculateReadTime(wordCount: number): number {\n  return Math.max(1, Math.ceil(wordCount / 200));\n}\n\n/**\n * Extract page content and metadata from DOM\n */\nexport function extractPageContent(document: Document, url?: string): PageMetadata {\n  const docUrl = url || document.location?.href || '';\n  const title = document.title || '';\n  \n  // Extract meta tags\n  const meta = extractMetaTags(document);\n  \n  // Extract headings\n  const headings = extractHeadings(document);\n  \n  // Find and extract main content\n  const mainContentElement = findMainContent(document);\n  const mainContent = mainContentElement ? extractCleanText(mainContentElement) : extractCleanText(document.body || document);\n  \n  // Extract structured data\n  const tables = extractTables(document);\n  const links = extractLinks(document);\n  const images = extractImages(document);\n  \n  // Calculate word count and read time\n  const wordCount = mainContent.split(/\\s+/).filter(Boolean).length;\n  const estimatedReadTime = calculateReadTime(wordCount);\n  \n  return {\n    title,\n    description: meta.description,\n    url: docUrl,\n    headings,\n    mainContent,\n    tables: tables.length > 0 ? tables : undefined,\n    links: links.length > 0 ? links : undefined,\n    images: images.length > 0 ? images : undefined,\n    author: meta.author,\n    publishedDate: meta.publishedDate,\n    wordCount,\n    estimatedReadTime,\n  };\n}\n\n/**\n * Extract page content from a URL (requires server-side or Electron context)\n * This is a placeholder - actual implementation depends on how you fetch pages\n */\nexport async function extractPageContentFromUrl(_url: string): Promise<PageMetadata | null> {\n  // This would typically use Electron's webContents or a headless browser\n  // For now, return null - implement based on your architecture\n  console.warn('[PageExtractor] extractPageContentFromUrl requires Electron or server context');\n  return null;\n}\n\n/**\n * Format extracted content for LLM consumption\n */\nexport function formatForLLM(metadata: PageMetadata, includeStructured = true): string {\n  let output = `# ${metadata.title}\\n\\n`;\n  \n  if (metadata.description) {\n    output += `**Description:** ${metadata.description}\\n\\n`;\n  }\n  \n  if (metadata.url) {\n    output += `**URL:** ${metadata.url}\\n\\n`;\n  }\n  \n  if (metadata.author) {\n    output += `**Author:** ${metadata.author}\\n`;\n  }\n  \n  if (metadata.publishedDate) {\n    output += `**Published:** ${metadata.publishedDate}\\n`;\n  }\n  \n  output += `\\n---\\n\\n`;\n  \n  // Add headings structure\n  if (metadata.headings.length > 0) {\n    output += `## Document Structure\\n\\n`;\n    for (const heading of metadata.headings) {\n      const prefix = '#'.repeat(heading.level);\n      output += `${prefix} ${heading.text}\\n`;\n    }\n    output += `\\n---\\n\\n`;\n  }\n  \n  // Add main content\n  output += `## Content\\n\\n${metadata.mainContent}\\n\\n`;\n  \n  // Add structured tables if requested\n  if (includeStructured && metadata.tables && metadata.tables.length > 0) {\n    output += `\\n---\\n\\n## Tables\\n\\n`;\n    for (const table of metadata.tables) {\n      if (table.headers.length > 0) {\n        output += `| ${table.headers.join(' | ')} |\\n`;\n        output += `| ${table.headers.map(() => '---').join(' | ')} |\\n`;\n      }\n      for (const row of table.rows) {\n        if (row.length > 0) {\n          output += `| ${row.join(' | ')} |\\n`;\n        }\n      }\n      output += `\\n`;\n    }\n  }\n  \n  return output;\n}\n\n"],"names":["extractCleanText","element","clone","el","text","extractHeadings","document","headings","headingElements","heading","level","extractTables","tables","tableElements","table","headers","rows","thead","headerRow","cells","cell","firstRow","dataRows","row","rowData","extractLinks","links","linkElements","link","href","url","extractImages","images","imageElements","img","src","alt","findMainContent","semanticSelectors","selector","body","largestElement","largestSize","candidates","candidate","role","className","id","extractMetaTags","meta","descTag","authorTag","dateTag","calculateReadTime","wordCount","extractPageContent","docUrl","title","mainContentElement","mainContent","estimatedReadTime"],"mappings":"AAmFA,SAASA,EAAiBC,EAAqC,CAE7D,MAAMC,EAAQD,EAAQ,UAAU,EAAI,EAGpBC,EAAM,iBAAiB,gDAAgD,EAC/E,QAAQC,GAAMA,EAAG,OAAA,CAAQ,EAGjC,IAAIC,EAAOF,EAAM,aAAe,GAGhC,OAAAE,EAAOA,EAAK,QAAQ,OAAQ,GAAG,EAAE,KAAA,EAGjCA,EAAOA,EAAK,QAAQ,WAAY,EAAE,EAClCA,EAAOA,EAAK,QAAQ,WAAY,EAAE,EAClCA,EAAOA,EAAK,QAAQ,OAAQ,GAAG,EAExBA,EAAK,KAAA,CACd,CAKA,SAASC,EAAgBC,EAA4D,CACnF,MAAMC,EAAmD,CAAA,EACnDC,EAAkBF,EAAS,iBAAiB,wBAAwB,EAE1E,UAAWG,KAAW,MAAM,KAAKD,CAAe,EAAG,CACjD,MAAME,EAAQ,SAASD,EAAQ,QAAQ,OAAO,CAAC,CAAC,EAC1CL,EAAOJ,EAAiBS,CAAO,EACjCL,GACFG,EAAS,KAAK,CAAE,MAAAG,EAAO,KAAAN,CAAA,CAAM,CAEjC,CAEA,OAAOG,CACT,CAKA,SAASI,EAAcL,EAAoE,CACzF,MAAMM,EAAyD,CAAA,EACzDC,EAAgBP,EAAS,iBAAiB,OAAO,EAEvD,UAAWQ,KAAS,MAAM,KAAKD,CAAa,EAAG,CAC7C,MAAME,EAAoB,CAAA,EACpBC,EAAmB,CAAA,EAGnBC,EAAQH,EAAM,cAAc,OAAO,EACzC,GAAIG,EAAO,CACT,MAAMC,EAAYD,EAAM,cAAc,IAAI,EAC1C,GAAIC,EAAW,CACb,MAAMC,EAAQD,EAAU,iBAAiB,QAAQ,EACjD,UAAWE,KAAQ,MAAM,KAAKD,CAAK,EACjCJ,EAAQ,KAAKf,EAAiBoB,CAAI,CAAC,CAEvC,CACF,KAAO,CAEL,MAAMC,EAAWP,EAAM,cAAc,IAAI,EACzC,GAAIO,EAAU,CACZ,MAAMF,EAAQE,EAAS,iBAAiB,QAAQ,EAChD,UAAWD,KAAQ,MAAM,KAAKD,CAAK,EACjCJ,EAAQ,KAAKf,EAAiBoB,CAAI,CAAC,CAEvC,CACF,CAIA,MAAME,GADQR,EAAM,cAAc,OAAO,GAAKA,GACvB,iBAAiB,IAAI,EAE5C,UAAWS,KAAO,MAAM,KAAKD,CAAQ,EAAG,CAEtC,GAAIL,GAASM,EAAI,gBAAkBN,EAAO,SAE1C,MAAME,EAAQI,EAAI,iBAAiB,IAAI,EACvC,GAAIJ,EAAM,OAAS,EAAG,CACpB,MAAMK,EAAU,MAAM,KAAKL,CAAK,EAAE,IAAKC,GAAkBpB,EAAiBoB,CAAI,CAAC,EAC/EJ,EAAK,KAAKQ,CAAO,CACnB,CACF,EAEIT,EAAQ,OAAS,GAAKC,EAAK,OAAS,IACtCJ,EAAO,KAAK,CAAE,QAAAG,EAAS,KAAAC,CAAA,CAAM,CAEjC,CAEA,OAAOJ,CACT,CAKA,SAASa,EAAanB,EAA0D,CAC9E,MAAMoB,EAA8C,CAAA,EAC9CC,EAAerB,EAAS,iBAAiB,SAAS,EAExD,UAAWsB,KAAQ,MAAM,KAAKD,CAAY,EAAG,CAC3C,MAAME,EAAOD,EAAK,aAAa,MAAM,EAC/BxB,EAAOJ,EAAiB4B,CAAI,EAElC,GAAIC,GAAQzB,EAEV,GAAI,CACF,MAAM0B,EAAM,IAAI,IAAID,EAAMvB,EAAS,SAAS,IAAI,EAAE,KAClDoB,EAAM,KAAK,CAAE,KAAAtB,EAAM,IAAA0B,CAAA,CAAK,CAC1B,MAAQ,CAER,CAEJ,CAEA,OAAOJ,CACT,CAKA,SAASK,EAAczB,EAA0D,CAC/E,MAAM0B,EAA+C,CAAA,EAC/CC,EAAgB3B,EAAS,iBAAiB,UAAU,EAE1D,UAAW4B,KAAO,MAAM,KAAKD,CAAa,EAAG,CAC3C,MAAME,EAAMD,EAAI,aAAa,KAAK,EAC5BE,EAAMF,EAAI,aAAa,KAAK,GAAK,OAEvC,GAAIC,EAEF,GAAI,CACF,MAAML,EAAM,IAAI,IAAIK,EAAK7B,EAAS,SAAS,IAAI,EAAE,KACjD0B,EAAO,KAAK,CAAE,IAAAI,EAAK,IAAKN,EAAK,CAC/B,MAAQ,CAER,CAEJ,CAEA,OAAOE,CACT,CAKA,SAASK,EAAgB/B,EAAoC,CAE3D,MAAMgC,EAAoB,CACxB,OACA,UACA,gBACA,WACA,gBACA,gBACA,iBACA,WACA,eAAA,EAGF,UAAWC,KAAYD,EAAmB,CACxC,MAAMrC,EAAUK,EAAS,cAAciC,CAAQ,EAC/C,GAAItC,EACF,OAAOA,CAEX,CAGA,MAAMuC,EAAOlC,EAAS,KACtB,GAAI,CAACkC,EAAM,OAAO,KAElB,IAAIC,EAAiC,KACjCC,EAAc,EAElB,MAAMC,EAAaH,EAAK,iBAAiB,cAAc,EACvD,UAAWI,KAAa,MAAM,KAAKD,CAAU,EAAG,CAE9C,MAAME,EAAOD,EAAU,aAAa,MAAM,EACpCE,EAAYF,EAAU,UAAU,YAAA,EAChCG,EAAKH,EAAU,GAAG,YAAA,EAExB,GACEC,IAAS,cACTA,IAAS,UACTA,IAAS,UACTC,EAAU,SAAS,KAAK,GACxBA,EAAU,SAAS,QAAQ,GAC3BA,EAAU,SAAS,QAAQ,GAC3BC,EAAG,SAAS,KAAK,GACjBA,EAAG,SAAS,QAAQ,GACpBA,EAAG,SAAS,QAAQ,EAEpB,SAGF,MAAM3C,EAAOJ,EAAiB4C,CAAS,EACnCxC,EAAK,OAASsC,IAChBA,EAActC,EAAK,OACnBqC,EAAiBG,EAErB,CAEA,OAAOH,GAAkBD,CAC3B,CAKA,SAASQ,EAAgB1C,EAIvB,CACA,MAAM2C,EAIF,CAAA,EAGEC,EAAU5C,EAAS,cAAc,2DAA2D,EAC9F4C,IACFD,EAAK,YAAcC,EAAQ,aAAa,SAAS,GAAK,QAIxD,MAAMC,EAAY7C,EAAS,cAAc,kDAAkD,EACvF6C,IACFF,EAAK,OAASE,EAAU,aAAa,SAAS,GAAK,QAIrD,MAAMC,EAAU9C,EAAS,cAAc,8FAA8F,EACrI,OAAI8C,IACFH,EAAK,cAAgBG,EAAQ,aAAa,UAAU,GAAKA,EAAQ,aAAa,SAAS,GAAK,QAGvFH,CACT,CAKA,SAASI,EAAkBC,EAA2B,CACpD,OAAO,KAAK,IAAI,EAAG,KAAK,KAAKA,EAAY,GAAG,CAAC,CAC/C,CAKO,SAASC,EAAmBjD,EAAoBwB,EAA4B,CACjF,MAAM0B,EAAS1B,GAAOxB,EAAS,UAAU,MAAQ,GAC3CmD,EAAQnD,EAAS,OAAS,GAG1B2C,EAAOD,EAAgB1C,CAAQ,EAG/BC,EAAWF,EAAgBC,CAAQ,EAGnCoD,EAAqBrB,EAAgB/B,CAAQ,EAC7CqD,EAAmC3D,EAArB0D,GAA6EpD,EAAS,MAAQA,CAAtC,EAGtEM,EAASD,EAAcL,CAAQ,EAC/BoB,EAAQD,EAAanB,CAAQ,EAC7B0B,EAASD,EAAczB,CAAQ,EAG/BgD,EAAYK,EAAY,MAAM,KAAK,EAAE,OAAO,OAAO,EAAE,OACrDC,EAAoBP,EAAkBC,CAAS,EAErD,MAAO,CACL,MAAAG,EACA,YAAaR,EAAK,YAClB,IAAKO,EACL,SAAAjD,EACA,YAAAoD,EACA,OAAQ/C,EAAO,OAAS,EAAIA,EAAS,OACrC,MAAOc,EAAM,OAAS,EAAIA,EAAQ,OAClC,OAAQM,EAAO,OAAS,EAAIA,EAAS,OACrC,OAAQiB,EAAK,OACb,cAAeA,EAAK,cACpB,UAAAK,EACA,kBAAAM,CAAA,CAEJ"}