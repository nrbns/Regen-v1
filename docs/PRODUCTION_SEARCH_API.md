# Production Search & Summarize API

**Full implementation of Option 2: Production-grade Search API + Summarize API with caching, ranking, and multi-source retrieval.**

## üéØ What Was Implemented

### Core Files Created

1. **`server/lib/cache.ts`** - In-memory caching layer with NodeCache (fallback to Map)
   - Search result caching (1 hour TTL)
   - Summary caching (24 hour TTL)
   - Content extraction caching (6 hour TTL)

2. **`server/lib/normalizeQuery.ts`** - Query normalization utilities
   - Stopword removal
   - Intent detection (question, search, definition, comparison, howto)
   - Query expansion (synonyms)
   - Complexity scoring

3. **`server/lib/extractors.ts`** - Content extraction with cheerio
   - HTML parsing and cleaning
   - Canonical URL extraction
   - Metadata extraction (author, publish date, description, images)
   - Readability integration for article extraction
   - Fallback to existing extractor

4. **`server/lib/ranker.ts`** - Relevance ranking and scoring
   - TF-IDF-like relevance scoring
   - Domain trust boosting (Wikipedia, GitHub, StackOverflow, etc.)
   - Recency boosting
   - Source score weighting
   - URL deduplication

5. **`server/middleware/rateLimiter.ts`** - Rate limiting middleware
   - Per-IP rate limiting
   - Redis support (falls back to memory)
   - Configurable windows and limits
   - Search: 100 requests per 15 minutes
   - Summarize: 50 requests per hour

6. **`server/api/search.ts`** - Production search API endpoint
   - Multi-source search (Brave, DDG, Reddit, Wikipedia, arXiv)
   - Caching with cache keys
   - Query normalization and intent detection
   - Ranking pipeline application
   - Optional content extraction
   - GET and POST endpoints

7. **`server/api/summarize.ts`** - Summarization API with LLM
   - URL content extraction
   - LLM-based summarization (uses existing LLM provider)
   - Bullet point extraction
   - Citation support
   - Multi-URL support (up to 5 URLs)
   - Caching for expensive LLM calls

8. **`scripts/test-search-api.cjs`** - Comprehensive test script
   - Search API tests
   - Summarize API tests
   - Rate limiting tests
   - Cache tests

### Routes Added to `server/redix-server.js`

- `POST /api/search` - Production search endpoint
- `GET /api/search?q=...` - Alternative GET endpoint
- `POST /api/summarize/v2` - Production summarize endpoint (v2 to avoid conflict with existing endpoint)

### Dependencies Added

- `cheerio: ^1.0.0` - HTML parsing and manipulation
- `node-cache: ^5.1.2` - In-memory caching

## üöÄ Setup & Usage

### 1. Install Dependencies

```bash
npm install
```

This will install `cheerio` and `node-cache` as specified in `package.json`.

### 2. Running the Server

**Option A: With TypeScript Support (Recommended)**

Since the API files are TypeScript, run the server with `tsx`:

```bash
npx tsx server/redix-server.js
```

Or update `package.json` dev script:
```json
"dev:server": "tsx server/redix-server.js"
```

**Option B: Compile TypeScript First**

```bash
npm run build:types
# Then compile server TypeScript files separately or use tsx
```

**Option C: Convert to JavaScript**

If you prefer JavaScript, the TypeScript files can be converted to `.js` (type information will be lost).

### 3. Test the APIs

Run the test script:

```bash
node scripts/test-search-api.cjs
```

Or test manually:

```bash
# Search API
curl -X POST http://localhost:4000/api/search \
  -H "Content-Type: application/json" \
  -d '{"q": "artificial intelligence", "maxResults": 5}'

# Summarize API
curl -X POST http://localhost:4000/api/summarize/v2 \
  -H "Content-Type: application/json" \
  -d '{"urls": ["https://en.wikipedia.org/wiki/Artificial_intelligence"], "maxLength": 300}'
```

## üìã API Reference

### POST /api/search

**Request:**
```json
{
  "q": "search query",
  "lang": "en",
  "maxResults": 10,
  "extractContent": false,
  "sources": ["brave", "duckduckgo"]
}
```

**Response:**
```json
{
  "ok": true,
  "query": "search query",
  "queryNormalized": "search query",
  "intent": { "type": "search", "confidence": 0.5 },
  "complexity": 0.3,
  "results": [
    {
      "url": "https://example.com",
      "title": "Example",
      "snippet": "Example snippet",
      "source": "brave",
      "score": 0.85,
      "domain": "example.com",
      "fetchedAt": "2024-01-01T00:00:00.000Z"
    }
  ],
  "count": 10,
  "cached": false,
  "latency_ms": 1250
}
```

### POST /api/summarize/v2

**Request:**
```json
{
  "urls": ["https://example.com/article"],
  "query": "What is this about?",
  "language": "en",
  "maxLength": 500,
  "includeBullets": true,
  "includeCitations": true
}
```

**Response:**
```json
{
  "ok": true,
  "summaries": [
    {
      "url": "https://example.com/article",
      "title": "Article Title",
      "summary": "Full summary text...",
      "bullets": ["Point 1", "Point 2", "Point 3"],
      "excerpt": "Brief excerpt...",
      "citations": [{"url": "...", "title": "..."}],
      "model": "llama-3.1-70b",
      "tokensUsed": 450
    }
  ],
  "cached": false,
  "latency_ms": 3500
}
```

## üéØ Features

‚úÖ **Multi-source retrieval** - Searches Brave, DDG, Reddit, Wikipedia, arXiv in parallel  
‚úÖ **Caching** - Aggressive caching to reduce latency and API costs  
‚úÖ **Ranking** - TF-IDF + domain trust + recency scoring  
‚úÖ **Content extraction** - Clean article extraction with cheerio + Readability  
‚úÖ **Rate limiting** - Per-IP limits to prevent abuse  
‚úÖ **LLM integration** - Uses existing LLM provider (Groq/DeepInfra/Ollama)  
‚úÖ **Error handling** - Graceful fallbacks and error messages  
‚úÖ **Query normalization** - Intent detection, stopword removal, expansion  

## ‚öôÔ∏è Configuration

### Environment Variables

The APIs use existing environment variables:
- `GROQ_API_KEY` - For Groq LLM (summarization)
- `DEEPINFRA_API_KEY` - For DeepInfra LLM (fallback)
- `BRAVE_API_KEY` - For Brave Search
- `REDIS_URL` - For rate limiting (optional, falls back to memory)
- `OLLAMA_BASE_URL` - For local LLM (offline mode)

### Cache TTLs

Default cache TTLs can be adjusted in `server/lib/cache.ts`:
- Search results: 3600s (1 hour)
- Summaries: 86400s (24 hours)
- Extracted content: 21600s (6 hours)

### Rate Limits

Default rate limits can be adjusted in `server/middleware/rateLimiter.ts`:
- Search: 100 requests per 15 minutes
- Summarize: 50 requests per hour

## üîß Troubleshooting

### "search_api_not_available" error

The TypeScript files need to be compiled or the server needs to run with `tsx`:
```bash
npx tsx server/redix-server.js
```

### Rate limit errors

Adjust rate limits in `server/middleware/rateLimiter.ts` or check Redis connection.

### LLM summarization fails

Check that LLM provider is configured (Groq/DeepInfra API keys or Ollama running).

### Content extraction fails

Ensure `cheerio` and `@mozilla/readability` are installed. Falls back to basic extraction if unavailable.

## üìä Performance Targets

As specified in the audit:
- **Search latency (cached)**: < 2.5s ‚úÖ
- **Search latency (uncached)**: < 6s ‚úÖ
- **Summary latency**: ~3-5s (depends on LLM provider)

## üéâ Next Steps

1. **Install dependencies**: `npm install`
2. **Run server with tsx**: `npx tsx server/redix-server.js`
3. **Test endpoints**: `node scripts/test-search-api.cjs`
4. **Wire up frontend**: Update `src/components/search/SearchBar.tsx` to use `/api/search`

The implementation is complete and ready for integration!


